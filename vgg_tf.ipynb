{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement VGG using TF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in layers =  ['conv5_1', 'fc6', 'conv5_3', 'fc7', 'fc8', 'conv5_2', 'conv4_1', 'conv4_2', 'conv4_3', 'conv3_3', 'conv3_2', 'conv3_1', 'conv1_1', 'conv1_2', 'conv2_2', 'conv2_1'] \n",
      "\n",
      "weights shape in conv1_1 (3, 3, 3, 64) \n",
      "\n",
      "bias shape in conv1_1 (64,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# load weights from https://dl.dropboxusercontent.com/u/50333326/vgg16.npy\n",
    "path = '/home/ariel/DL/tensorflow/tutorials/'\n",
    "vgg16_npy_path = os.path.join(path, 'vgg16.npy')            \n",
    "data_dict = np.load(vgg16_npy_path, encoding='latin1').item()\n",
    "\n",
    "# print keys in dictionary - \n",
    "# here it prints all the layers names in the VGG net\n",
    "name = 'conv1_1'\n",
    "print 'keys in layers = ', data_dict.keys(), '\\n'\n",
    "# tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "print 'weights shape in conv1_1', data_dict[name][0].shape, '\\n'\n",
    "print 'bias shape in conv1_1', data_dict[name][1].shape, '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Conv Block in TF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _max_pool(bottom, name):\n",
    "    pool = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                          padding='SAME', name=name)\n",
    "    return pool\n",
    "\n",
    "def _bias_reshape(bweight, num_orig, num_new):\n",
    "    \"\"\" Build bias weights for filter produces with `_summary_reshape`\n",
    "    \"\"\"\n",
    "    n_averaged_elements = num_orig//num_new\n",
    "    avg_bweight = np.zeros(num_new)\n",
    "    for i in range(0, num_orig, n_averaged_elements):\n",
    "        start_idx = i\n",
    "        end_idx = start_idx + n_averaged_elements\n",
    "        avg_idx = start_idx//n_averaged_elements\n",
    "        if avg_idx == num_new:\n",
    "            break\n",
    "        avg_bweight[avg_idx] = np.mean(bweight[start_idx:end_idx])\n",
    "    return avg_bweight\n",
    "\n",
    "# need to understand this! Only for fc8, the bias is recalculated!\n",
    "def get_bias(name, num_classes=None):\n",
    "    bias_wights = data_dict[name][1]\n",
    "    shape = data_dict[name][1].shape\n",
    "    if name == 'fc8':\n",
    "        bias_wights = _bias_reshape(bias_wights, shape[0], num_classes)\n",
    "        shape = [num_classes]\n",
    "    init = tf.constant_initializer(value=bias_wights, dtype=tf.float32)\n",
    "    return tf.get_variable(name=\"biases\", initializer=init, shape=shape)\n",
    "\n",
    "def get_conv_filter(name):\n",
    "    with tf.variable_scope(\"C\"):\n",
    "        init = tf.constant_initializer(value=data_dict[name][0], dtype=tf.float32)\n",
    "        shape = data_dict[name][0].shape\n",
    "        print('Layer name: %s' % name)\n",
    "        print('Layer shape: %s' % str(shape))\n",
    "        var = tf.get_variable(name=\"filter\", initializer=init, shape=shape)\n",
    "#     if not tf.get_variable_scope().reuse:\n",
    "#         weight_decay = tf.multiply(tf.nn.l2_loss(var), self.wd,\n",
    "#                                    name='weight_loss')\n",
    "#         tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES,\n",
    "#                              weight_decay)\n",
    "    return var\n",
    "\n",
    "def get_weights(name):\n",
    "    init = tf.constant_initializer(value=data_dict[name][0], dtype=tf.float32)\n",
    "    weights = tf.get_variable(name=\"weights\", initializer=init, shape=data_dict[name][0].shape)\n",
    "    return weights\n",
    "\n",
    "def get_biases(name):\n",
    "    init = tf.constant_initializer(value=data_dict[name][1], dtype=tf.float32)\n",
    "    biases = tf.get_variable(name=\"biases\", initializer=init, shape=data_dict[name][1].shape)\n",
    "    return biases\n",
    "\n",
    "def _conv_layer(bottom, name):\n",
    "    with tf.variable_scope(name):# as scope:\n",
    "        # get filter weights\n",
    "        weights = get_weights(name)\n",
    "       \n",
    "        # get filter biases\n",
    "        biases = get_biases(name)\n",
    "\n",
    "        # convolve: weights * bottom\n",
    "        conv = tf.nn.conv2d(bottom, weights, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        # add biases: weights * bottom + biases\n",
    "        conv_biases = tf.nn.bias_add(conv, biases)\n",
    "\n",
    "        # relu: relu(weights * bottom + biases)\n",
    "        conv_biases_relu = tf.nn.relu(conv_biases)\n",
    "\n",
    "        return conv_biases_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** the network **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 512)\n",
      "[[[[ 0.04152922  0.          0.         ...,  0.          0.76243585  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.73776311  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.73881972  0.        ]\n",
      "   ..., \n",
      "   [ 0.          0.          0.         ...,  0.          0.72844511  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.79345977  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.78591818  0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...,  0.          0.67036343  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.70792282  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.72337979  0.        ]\n",
      "   ..., \n",
      "   [ 0.          0.          0.         ...,  0.          0.72094685  0.        ]\n",
      "   [ 0.06755218  0.          0.         ...,  0.          0.66223222  0.        ]\n",
      "   [ 0.07043768  0.          0.         ...,  0.          0.69173503  0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...,  0.          0.67313582  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.7398631   0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.74472582  0.        ]\n",
      "   ..., \n",
      "   [ 0.          0.          0.         ...,  0.          0.74327683  0.        ]\n",
      "   [ 0.0562786   0.          0.         ...,  0.          0.67947733  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.64110082  0.        ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.          0.          0.         ...,  0.          0.683025    0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.71301287  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.71626157  0.        ]\n",
      "   ..., \n",
      "   [ 0.          0.          0.         ...,  0.          0.71500897  0.        ]\n",
      "   [ 0.03658648  0.          0.         ...,  0.          0.63591945  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.62517077  0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.         ...,  0.          0.74236947  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.65076977  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.66111845  0.        ]\n",
      "   ..., \n",
      "   [ 0.          0.          0.         ...,  0.          0.662561    0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.57818824  0.        ]\n",
      "   [ 0.          0.          0.         ...,  0.          0.58536279  0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.10145122 ...,  0.          0.84740597  0.        ]\n",
      "   [ 0.          0.          0.04614043 ...,  0.          0.72118312  0.        ]\n",
      "   [ 0.          0.          0.04577801 ...,  0.          0.6965946   0.        ]\n",
      "   ..., \n",
      "   [ 0.          0.          0.0362125  ...,  0.          0.70292878  0.        ]\n",
      "   [ 0.          0.          0.01458383 ...,  0.          0.67077625  0.        ]\n",
      "   [ 0.          0.          0.06460804 ...,  0.          0.59977245  0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "# this command clean all nodes in the graph + variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# load an image\n",
    "IMAGE_SIZE = 224\n",
    "img_shape = (1, IMAGE_SIZE, IMAGE_SIZE, 3) # tf format (N,H,W,C)\n",
    "img = np.linspace(-0.1, 0.5, num=np.prod(img_shape)).reshape(img_shape)\n",
    "tf_img = tf.Variable(img, name=\"img\", dtype=tf.float32)\n",
    "\n",
    "_conv1_1 = _conv_layer(tf_img, 'conv1_1')\n",
    "_conv1_2 = _conv_layer(_conv1_1, 'conv1_2')\n",
    "_pool1   = _max_pool(_conv1_2, 'pool1')\n",
    "\n",
    "_conv2_1 = _conv_layer(_pool1, 'conv2_1')\n",
    "_conv2_2 = _conv_layer(_conv2_1, 'conv2_2')\n",
    "_pool2   = _max_pool(_conv2_2, 'pool2')\n",
    "\n",
    "_conv3_1 = _conv_layer(_pool2, 'conv3_1')\n",
    "_conv3_2 = _conv_layer(_conv3_1, 'conv3_2')\n",
    "_conv3_3 = _conv_layer(_conv3_2, 'conv3_3')\n",
    "_pool3   = _max_pool(_conv3_3, 'pool3')\n",
    "\n",
    "_conv4_1 = _conv_layer(_pool3, 'conv4_1')\n",
    "_conv4_2 = _conv_layer(_conv4_1, 'conv4_2')\n",
    "_conv4_3 = _conv_layer(_conv4_2, 'conv4_3')\n",
    "_pool4   = _max_pool(_conv4_3, 'pool4')\n",
    "\n",
    "_conv5_1 = _conv_layer(_pool4, 'conv5_1')\n",
    "_conv5_2 = _conv_layer(_conv5_1, 'conv5_2')\n",
    "_conv5_3 = _conv_layer(_conv5_2, 'conv5_3')\n",
    "_pool5   = _max_pool(_conv5_3, 'pool5')\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "res = sess.run(_pool5) \n",
    "print res.shape\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
